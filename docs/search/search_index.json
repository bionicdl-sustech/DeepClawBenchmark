{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The DeepClaw Benchmark \u00b6 The DeepClaw is a benchmarking model zoo that functions as a Reconfigurable Robotic Manipulation System for Robot Learning. The main homepage can be found at here . This is the GitHub repository of DeepClaw source code, including instructions for installing and using DeepClaw, below. Resources \u00b6 Documentation: https://bionicdl-sustech.github.io/DeepClawBenchmark/ Paper explaining DeepClaw: arXiv:2005.02588 [cs.RO] Papers using DeepClaw: arXiv:2003.01584 [cs.RO] arXiv:2003.01583 [cs.RO] arXiv:2003.01582 [cs.RO] Code Organization \u00b6 The DeepClaw code is organized as follows: configs/ configuration for robotic station for manipulation tasks. deepclaw/drivers/ drivers for various robotic hardware, i.e. ur, franka, aubo. deepclaw/models/ model zoo for segmentation, classification, pick planning, and motion planning. deepclaw/utils/ server setup with dockers and client setup for laptops (x86) and jetson (arm). projects/proj_TrashSorting a sample project to run deepclaw for sorting trash. datasets/trash description of trash sorting dataset docs/ description of this document as a manual. data/trash data on trash sorting Bibliography \u00b6 @misc{wan2020deepclaw, title={DeepClaw: A Robotic Hardware Benchmarking Platform for Learning Object Manipulation}, author={Fang Wan and Haokun Wang and Xiaobo Liu and Linhan Yang and Chaoyang Song}, year={2020}, eprint={2005.02588}, archivePrefix={arXiv}, primaryClass={cs.RO} }","title":"Home"},{"location":"#the-deepclaw-benchmark","text":"The DeepClaw is a benchmarking model zoo that functions as a Reconfigurable Robotic Manipulation System for Robot Learning. The main homepage can be found at here . This is the GitHub repository of DeepClaw source code, including instructions for installing and using DeepClaw, below.","title":"The DeepClaw Benchmark"},{"location":"#resources","text":"Documentation: https://bionicdl-sustech.github.io/DeepClawBenchmark/ Paper explaining DeepClaw: arXiv:2005.02588 [cs.RO] Papers using DeepClaw: arXiv:2003.01584 [cs.RO] arXiv:2003.01583 [cs.RO] arXiv:2003.01582 [cs.RO]","title":"Resources"},{"location":"#code-organization","text":"The DeepClaw code is organized as follows: configs/ configuration for robotic station for manipulation tasks. deepclaw/drivers/ drivers for various robotic hardware, i.e. ur, franka, aubo. deepclaw/models/ model zoo for segmentation, classification, pick planning, and motion planning. deepclaw/utils/ server setup with dockers and client setup for laptops (x86) and jetson (arm). projects/proj_TrashSorting a sample project to run deepclaw for sorting trash. datasets/trash description of trash sorting dataset docs/ description of this document as a manual. data/trash data on trash sorting","title":"Code Organization"},{"location":"#bibliography","text":"@misc{wan2020deepclaw, title={DeepClaw: A Robotic Hardware Benchmarking Platform for Learning Object Manipulation}, author={Fang Wan and Haokun Wang and Xiaobo Liu and Linhan Yang and Chaoyang Song}, year={2020}, eprint={2005.02588}, archivePrefix={arXiv}, primaryClass={cs.RO} }","title":"Bibliography"},{"location":"about/","text":"Scyrum an prima rubent poteratque ulla celebrare \u00b6 Orba formosa \u00b6 Lorem markdownum inventa vos quis sinistra moventur harundine nubibus, in movere dedit sacer magnas opis aetatis! Patitur has freta pulchros centumque, ubi altera dearum. Illae venit gerit ortum inpia ingens, femina forent tremulis; victor uror putares prolem . Fragmina si ubi dea genuit es viso quem, vetustas et quid, rex contulerat lentisciferumque insilit. Trepidus locutus corpora ignes! Nec enim fecere Nam tarda et alter non Ubi regni solebat Gentes viscera precor rogat In clausit patrem quamvis verbere Nunc ore mater armenta est lumine, quam ter Coniugis a cumque, amictu, atra nudos fugiunt Achivi geminos. Nec est Curibusque sperni saxoque et raptae, nulli! Tibi sub adest magnis agnovit quaecumque somnos flammas inpia concutiens, conscendit At cum prohibete. Tibi icta saevior simillimus loqui; sed herba ira erat viribus lumen pendeat modo senex porrigeret quaesita mea; arboreo. Ululatibus primo renasci \u00b6 Pars edentem sulphura nati prodest, ego quo, atrae inpius saxum. Et e ramos volucrem volenti ritu, inserere domos fronti famam. Sidere esse lumina Orphne Somnia donisque liquerat dixit et agrestis ergo, mente tamen. Est est Domoque vocat Tibi tecum sistetur Cecrope natos Phegeius Dolos fulmina Vita sub violave quae Pavidus o in alis \u00b6 Simul si nec fallebat fraterno parenti in culpa matertera inde planxere enim Ciconum ne tempora meorum praecipitem mersit. Obscuraque daedalus Iunone; dulce in quantas erat sumit animae cui undis ad latronis, inpune et. Sagitta virgo turpius: ursos quis spiritus pisa vatis fatale, mortis vidisti, in, sed Hactenus pendet. In nubimus qui, Somnia cecidisse alantqueimperat quaeritur contende alimentaque et in resolvit habentem tantos putes: femina levis suus? Labori et meorum fuissem membris. Omnes Proserpina vocassent apro excubias ulvaeque Pleuronius et mugiat modo placat quod. Ter thalamos relinque, putes tegis partem ferat mihi , nullamque saepe, virosque caelesti sub et. Latratibus notissima violenta ad veri , sub de est, de formosi adspicere plus orbem conviciaque videtur. Frater dederat nunc, meritoque urebat et sed; genialis draconi certamine flagrantem tabo; Mermeros. Lumen tu sed rapinae lotos hominis ter, erat dempto in aevum nimiumque ex neci simul manusque Tarpeias callem insistere. Virgo non quibus tamen rursus, Patareaque, nec reppulit putes et septem.","title":"Scyrum an prima rubent poteratque ulla celebrare"},{"location":"about/#scyrum-an-prima-rubent-poteratque-ulla-celebrare","text":"","title":"Scyrum an prima rubent poteratque ulla celebrare"},{"location":"about/#orba-formosa","text":"Lorem markdownum inventa vos quis sinistra moventur harundine nubibus, in movere dedit sacer magnas opis aetatis! Patitur has freta pulchros centumque, ubi altera dearum. Illae venit gerit ortum inpia ingens, femina forent tremulis; victor uror putares prolem . Fragmina si ubi dea genuit es viso quem, vetustas et quid, rex contulerat lentisciferumque insilit. Trepidus locutus corpora ignes! Nec enim fecere Nam tarda et alter non Ubi regni solebat Gentes viscera precor rogat In clausit patrem quamvis verbere Nunc ore mater armenta est lumine, quam ter Coniugis a cumque, amictu, atra nudos fugiunt Achivi geminos. Nec est Curibusque sperni saxoque et raptae, nulli! Tibi sub adest magnis agnovit quaecumque somnos flammas inpia concutiens, conscendit At cum prohibete. Tibi icta saevior simillimus loqui; sed herba ira erat viribus lumen pendeat modo senex porrigeret quaesita mea; arboreo.","title":"Orba formosa"},{"location":"about/#ululatibus-primo-renasci","text":"Pars edentem sulphura nati prodest, ego quo, atrae inpius saxum. Et e ramos volucrem volenti ritu, inserere domos fronti famam. Sidere esse lumina Orphne Somnia donisque liquerat dixit et agrestis ergo, mente tamen. Est est Domoque vocat Tibi tecum sistetur Cecrope natos Phegeius Dolos fulmina Vita sub violave quae","title":"Ululatibus primo renasci"},{"location":"about/#pavidus-o-in-alis","text":"Simul si nec fallebat fraterno parenti in culpa matertera inde planxere enim Ciconum ne tempora meorum praecipitem mersit. Obscuraque daedalus Iunone; dulce in quantas erat sumit animae cui undis ad latronis, inpune et. Sagitta virgo turpius: ursos quis spiritus pisa vatis fatale, mortis vidisti, in, sed Hactenus pendet. In nubimus qui, Somnia cecidisse alantqueimperat quaeritur contende alimentaque et in resolvit habentem tantos putes: femina levis suus? Labori et meorum fuissem membris. Omnes Proserpina vocassent apro excubias ulvaeque Pleuronius et mugiat modo placat quod. Ter thalamos relinque, putes tegis partem ferat mihi , nullamque saepe, virosque caelesti sub et. Latratibus notissima violenta ad veri , sub de est, de formosi adspicere plus orbem conviciaque videtur. Frater dederat nunc, meritoque urebat et sed; genialis draconi certamine flagrantem tabo; Mermeros. Lumen tu sed rapinae lotos hominis ter, erat dempto in aevum nimiumque ex neci simul manusque Tarpeias callem insistere. Virgo non quibus tamen rursus, Patareaque, nec reppulit putes et septem.","title":"Pavidus o in alis"},{"location":"code-api/","text":"API Reference \u00b6 The deepclaw library API consists of following parts: Hardware Driver API Arm Hand Camera Modules Pool API Segmentation Recognition Grasp Planning Motion Planning utils API Hardware Driver API \u00b6 The Hardware Driver API is used for controling the Hardware. Arm \u00b6 UR10e \u00b6 class deepclaw.driver.arms. UR10eController (robot_configuration_file_path) parameters[in]: robot_configuration_file_path, the robot configuration file, and the file format is yaml. return: an instance of UR10e controller class Methods go_home() Description : move to pre-defined joints, the home joints is defined in robot configuration file. move_j(joints_angle, velocity=None, acceleration=None, solution_space='Joint') Description : go to the target joints positions param[in] : - joints_angle : target joint positions of each joints [rad]; - velocity : joint acceleration of leading axis [rad/s]; - accelerate : joint speed of leading axis [rad/s^2]; - solution_space : move style, 'Joint' means it linear in joint-space(inverse kinematics is used to calculate the corresponding joints), and 'Space' means linear in tool-space return : bool, reaching target or not move_p(position, velocity=None, acceleration=None,solution_space='Joint') Description : go to the target pose(Rotation vector) param[in] : - position : target pose; - velocity : joint acceleration of leading axis [rad/s^2]; - accelerate : joint speed of leading axis [rad/s]; - solution_space : move style, 'Joint' means it linear in joint-space,and 'Space' means linear in tool-space(forward kinematics is used to calculate the corresponding pose) return : bool, reaching target or not get_state() Description : get robot state return : dictionary, the whole states of the UR10e verify_state(variable_name, target_value, error=0.0001, time_out=10) Description : verify the robot reaching the target pose(joint or cartesian) or not param[in] : - variable_name : target style, joints('q_actual') or cartesian('tool_vector_actual'); - target_value : target values; - error : threshold,if the difference between current state and target state is small than threshold, we say the robot reached the target; - time_out : Max time of the motion, [second] return : bool, reaching target or not Hand \u00b6 HandE \u00b6 class deepclaw.driver.grippers.handE_controller. HandEController (robot_ip = \"192.168.1.10\",port = 30003) parameters[in]: robot_ip, the UR ip which the gripper mounted on; port, the UR ip. return: an instance of gripper controller class Methods close_gripper() Description : close the gripper. open_gripper() Description : open the gripper. Camera \u00b6 Realsense D435 \u00b6 class deepclaw.driver.sensors.cameras. Realsense (camera_configuration_file_path) parameters[in]: camera_configuration_file_path, the camera configuration file, and the file format is yaml. return: an instance of camera controller class Methods get_frame() Description : get images from a camera. return : a tuple, includes color image, depth image, point cloud, and infrared images. get_intrinsics() Description : get intrinsics attributes of a camera. return : a tuple, intrinsics parameters of the camera Modules Pool API \u00b6 Calibration \u00b6 deepclaw.modules.calibration.EyeOnBase.Calibration \u00b6 class deepclaw.modules.calibration.EyeOnBase. Calibration (arm, camera, configuration_file) Parameters arm camera configuration_file Methods run (self): Calibrate robot arm and camera End-to-End \u00b6 deepclaw.modules.end2end.effecientdet.efficientdet_predictor.efficientdet \u00b6 class deepclaw.modules.end2end.effecientdet.efficientdet_predictor. efficientdet (compound_coef=0, weight_path=None, num_classes=204) Parameters compound_coef weight_path num_classes Methods run (self, image_np) display (self, preds, imgs, save_path, imshow=False, imwrite=True) Segmentation \u00b6 deepclaw.modules.segmentation.ContourDetector.ContourDetector \u00b6 class deepclaw.modules.segmentation.ContourDetector.ContourDetector (mode=cv2.RETR_LIST, method=cv2.CHAIN_APPROX_NONE, binary_threshold=127, area_threshold=(80, 200), with_angle=False) Parameter - mode - method - binary_threshold - area_threshold - with_angle Methods run (self, color_image)","title":"deepclaw"},{"location":"code-api/#api-reference","text":"The deepclaw library API consists of following parts: Hardware Driver API Arm Hand Camera Modules Pool API Segmentation Recognition Grasp Planning Motion Planning utils API","title":"API Reference"},{"location":"code-api/#hardware-driver-api","text":"The Hardware Driver API is used for controling the Hardware.","title":"Hardware Driver API"},{"location":"code-api/#arm","text":"","title":"Arm"},{"location":"code-api/#ur10e","text":"class deepclaw.driver.arms. UR10eController (robot_configuration_file_path) parameters[in]: robot_configuration_file_path, the robot configuration file, and the file format is yaml. return: an instance of UR10e controller class Methods go_home() Description : move to pre-defined joints, the home joints is defined in robot configuration file. move_j(joints_angle, velocity=None, acceleration=None, solution_space='Joint') Description : go to the target joints positions param[in] : - joints_angle : target joint positions of each joints [rad]; - velocity : joint acceleration of leading axis [rad/s]; - accelerate : joint speed of leading axis [rad/s^2]; - solution_space : move style, 'Joint' means it linear in joint-space(inverse kinematics is used to calculate the corresponding joints), and 'Space' means linear in tool-space return : bool, reaching target or not move_p(position, velocity=None, acceleration=None,solution_space='Joint') Description : go to the target pose(Rotation vector) param[in] : - position : target pose; - velocity : joint acceleration of leading axis [rad/s^2]; - accelerate : joint speed of leading axis [rad/s]; - solution_space : move style, 'Joint' means it linear in joint-space,and 'Space' means linear in tool-space(forward kinematics is used to calculate the corresponding pose) return : bool, reaching target or not get_state() Description : get robot state return : dictionary, the whole states of the UR10e verify_state(variable_name, target_value, error=0.0001, time_out=10) Description : verify the robot reaching the target pose(joint or cartesian) or not param[in] : - variable_name : target style, joints('q_actual') or cartesian('tool_vector_actual'); - target_value : target values; - error : threshold,if the difference between current state and target state is small than threshold, we say the robot reached the target; - time_out : Max time of the motion, [second] return : bool, reaching target or not","title":"UR10e"},{"location":"code-api/#hand","text":"","title":"Hand"},{"location":"code-api/#hande","text":"class deepclaw.driver.grippers.handE_controller. HandEController (robot_ip = \"192.168.1.10\",port = 30003) parameters[in]: robot_ip, the UR ip which the gripper mounted on; port, the UR ip. return: an instance of gripper controller class Methods close_gripper() Description : close the gripper. open_gripper() Description : open the gripper.","title":"HandE"},{"location":"code-api/#camera","text":"","title":"Camera"},{"location":"code-api/#realsense-d435","text":"class deepclaw.driver.sensors.cameras. Realsense (camera_configuration_file_path) parameters[in]: camera_configuration_file_path, the camera configuration file, and the file format is yaml. return: an instance of camera controller class Methods get_frame() Description : get images from a camera. return : a tuple, includes color image, depth image, point cloud, and infrared images. get_intrinsics() Description : get intrinsics attributes of a camera. return : a tuple, intrinsics parameters of the camera","title":"Realsense D435"},{"location":"code-api/#modules-pool-api","text":"","title":"Modules Pool API"},{"location":"code-api/#calibration","text":"","title":"Calibration"},{"location":"code-api/#deepclawmodulescalibrationeyeonbasecalibration","text":"class deepclaw.modules.calibration.EyeOnBase. Calibration (arm, camera, configuration_file) Parameters arm camera configuration_file Methods run (self): Calibrate robot arm and camera","title":"deepclaw.modules.calibration.EyeOnBase.Calibration"},{"location":"code-api/#end-to-end","text":"","title":"End-to-End"},{"location":"code-api/#deepclawmodulesend2endeffecientdetefficientdet_predictorefficientdet","text":"class deepclaw.modules.end2end.effecientdet.efficientdet_predictor. efficientdet (compound_coef=0, weight_path=None, num_classes=204) Parameters compound_coef weight_path num_classes Methods run (self, image_np) display (self, preds, imgs, save_path, imshow=False, imwrite=True)","title":"deepclaw.modules.end2end.effecientdet.efficientdet_predictor.efficientdet"},{"location":"code-api/#segmentation","text":"","title":"Segmentation"},{"location":"code-api/#deepclawmodulessegmentationcontourdetectorcontourdetector","text":"class deepclaw.modules.segmentation.ContourDetector.ContourDetector (mode=cv2.RETR_LIST, method=cv2.CHAIN_APPROX_NONE, binary_threshold=127, area_threshold=(80, 200), with_angle=False) Parameter - mode - method - binary_threshold - area_threshold - with_angle Methods run (self, color_image)","title":"deepclaw.modules.segmentation.ContourDetector.ContourDetector"},{"location":"data/","text":"data \u00b6 Deepclaw maintains a collection of public datasets for benchamrking purposes. MNIST \u00b6 The MNIST database is a handwritten digits data set, has a training set of 60,000 examples, and a test set of 10,000 examples. The digits have been size-normalized and centered in a fixed-size image (28x28). There are 4 files in the dateset: training set images, training set labels, test set images, and test set labels. See more details and download in here . Fashion-MNIST \u00b6 Fashion-MNIST is a fashion products data set from 10 categories, consists of a training set of 60,000 examples and a test set of 10,000 examples. Go to the github page for more detailed information and downloading data set. CIFAR \u00b6 The CIFAR-10 and CIFAR-100 are labeled subsets of the 80 million tiny images dataset. They were collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. See more details and download in here . Haihua-Trash-Sorting \u00b6 The Dataset is provided by the 2020 Haihua AI Challenge\u00b7Garbage Classification. Please visit the website for more information on the competition. The original training dataset provides 80,000 images containing a single type of trash in each image (simple data) and 2998 images containing multiple types of trashes (up to 20 types) in each image (complex data). Each image is 1920x1080 in size. The labels provide the bounding boxes and the corresponding classification labels. Beside, the competition also provides 10000 simple images and 1000 complex images containing multiple types of trashes for testing without labels. There are 204 classes of trashes in total. Due to the size of the original dataset (180G simple data and 18G complex data), it is quite challenge to download the full dataset. Hence we extract part of the simple dataset which belong to the following four recyclable material: glass, paper, metal and plastics. The recyclable dataset contains 14122 images of a single trash in each image. Images are resized to 960x540, which is half of the original resolution. There are four categories and the original category id is kept as category_id_ori in the label file. To be added \u00b6","title":"Common Dataset"},{"location":"data/#data","text":"Deepclaw maintains a collection of public datasets for benchamrking purposes.","title":"data"},{"location":"data/#mnist","text":"The MNIST database is a handwritten digits data set, has a training set of 60,000 examples, and a test set of 10,000 examples. The digits have been size-normalized and centered in a fixed-size image (28x28). There are 4 files in the dateset: training set images, training set labels, test set images, and test set labels. See more details and download in here .","title":"MNIST"},{"location":"data/#fashion-mnist","text":"Fashion-MNIST is a fashion products data set from 10 categories, consists of a training set of 60,000 examples and a test set of 10,000 examples. Go to the github page for more detailed information and downloading data set.","title":"Fashion-MNIST"},{"location":"data/#cifar","text":"The CIFAR-10 and CIFAR-100 are labeled subsets of the 80 million tiny images dataset. They were collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. See more details and download in here .","title":"CIFAR"},{"location":"data/#haihua-trash-sorting","text":"The Dataset is provided by the 2020 Haihua AI Challenge\u00b7Garbage Classification. Please visit the website for more information on the competition. The original training dataset provides 80,000 images containing a single type of trash in each image (simple data) and 2998 images containing multiple types of trashes (up to 20 types) in each image (complex data). Each image is 1920x1080 in size. The labels provide the bounding boxes and the corresponding classification labels. Beside, the competition also provides 10000 simple images and 1000 complex images containing multiple types of trashes for testing without labels. There are 204 classes of trashes in total. Due to the size of the original dataset (180G simple data and 18G complex data), it is quite challenge to download the full dataset. Hence we extract part of the simple dataset which belong to the following four recyclable material: glass, paper, metal and plastics. The recyclable dataset contains 14122 images of a single trash in each image. Images are resized to 960x540, which is half of the original resolution. There are four categories and the original category id is kept as category_id_ori in the label file.","title":"Haihua-Trash-Sorting"},{"location":"data/#to-be-added","text":"","title":"To be added"},{"location":"devnote/","text":"Developer Note \u00b6 This documentation is edited using MkDocs . For full documentation visit mkdocs.org . Commands \u00b6 mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Adding Pages \u00b6 Because the documentation will include some navigation headers, you may need to edit the configuration file first. The mkdocs.yaml configuration file looks like: site_name : DeepClaw nav : - Home : - Overview : index.md - Installation : install.md - Manual : - DeepClaw Pipeline : pipeline.md - ... - API : - deepclaw : code-api.md - Notes : - For Developers : devnote.md theme : readthedocs You can add a new markdown file into docs, like docs/<NEW_FILE_NAME>.md . Then edit this configuration file, insert the markdown file into the nav , for example: ... nav : ... - Manual : - <NAVIGATION TAG> : <NEW_FILE_NAME>.md - ... - ... Build & Display \u00b6 CD into DEEPCLAW_ROOT/doc/deepclawdoc/ $ cd doc/deepclawdoc/ Then start MKDOCS serve $ mkdocs serve INFO - Building documentation... INFO - Cleaning site directory [I 160402 15:50:43 server:271] Serving on http://127.0.0.1:8000 [I 160402 15:50:43 handlers:58] Start watching changes [I 160402 15:50:43 handlers:60] Start detecting changes Open up http:/127.0.0.1:8000/ in your browser, and you'll see the default home page being displayed.","title":"For Developers"},{"location":"devnote/#developer-note","text":"This documentation is edited using MkDocs . For full documentation visit mkdocs.org .","title":"Developer Note"},{"location":"devnote/#commands","text":"mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"devnote/#adding-pages","text":"Because the documentation will include some navigation headers, you may need to edit the configuration file first. The mkdocs.yaml configuration file looks like: site_name : DeepClaw nav : - Home : - Overview : index.md - Installation : install.md - Manual : - DeepClaw Pipeline : pipeline.md - ... - API : - deepclaw : code-api.md - Notes : - For Developers : devnote.md theme : readthedocs You can add a new markdown file into docs, like docs/<NEW_FILE_NAME>.md . Then edit this configuration file, insert the markdown file into the nav , for example: ... nav : ... - Manual : - <NAVIGATION TAG> : <NEW_FILE_NAME>.md - ... - ...","title":"Adding Pages"},{"location":"devnote/#build-display","text":"CD into DEEPCLAW_ROOT/doc/deepclawdoc/ $ cd doc/deepclawdoc/ Then start MKDOCS serve $ mkdocs serve INFO - Building documentation... INFO - Cleaning site directory [I 160402 15:50:43 server:271] Serving on http://127.0.0.1:8000 [I 160402 15:50:43 handlers:58] Start watching changes [I 160402 15:50:43 handlers:60] Start detecting changes Open up http:/127.0.0.1:8000/ in your browser, and you'll see the default home page being displayed.","title":"Build &amp; Display"},{"location":"install/","text":"Installation \u00b6 The DeepClaw is a benchmarking module pool that functions as a Reconfigurable Robotic Manipulation System for Robot Learning. The main homepage for Julia can be found at deepclaw.ancorasir.com . This is the GitHub repository of DeepClaw source code, including instructions for installing and using DeepClaw, below. Requirements \u00b6 In the current release, support is provided for a baseline setup with UR10e, HandE, and RealSense D435. The dependencies of DeepClaw are showed below: RealSense SDK ( how to install ) Python packages numpy PyYAML pyrealsense2 Before using DeepClaw, please ensure you have installed all above packages. The python packages numpy , PyYaml , and pyrealsense2 will be installed when isntalling DeepClaw using pip. Install using pip \u00b6 Install DeepClaw using pip in virtual environment. python3 -m pip install DeepClaw Install from source \u00b6 Clone repository from Github. git clone https://github.com/bionicdl-sustech/DeepClawBenchmark.git cd DeepClawBenchmark Install DeepClaw. python3 setup.py install","title":"Installation"},{"location":"install/#installation","text":"The DeepClaw is a benchmarking module pool that functions as a Reconfigurable Robotic Manipulation System for Robot Learning. The main homepage for Julia can be found at deepclaw.ancorasir.com . This is the GitHub repository of DeepClaw source code, including instructions for installing and using DeepClaw, below.","title":"Installation"},{"location":"install/#requirements","text":"In the current release, support is provided for a baseline setup with UR10e, HandE, and RealSense D435. The dependencies of DeepClaw are showed below: RealSense SDK ( how to install ) Python packages numpy PyYAML pyrealsense2 Before using DeepClaw, please ensure you have installed all above packages. The python packages numpy , PyYaml , and pyrealsense2 will be installed when isntalling DeepClaw using pip.","title":"Requirements"},{"location":"install/#install-using-pip","text":"Install DeepClaw using pip in virtual environment. python3 -m pip install DeepClaw","title":"Install using pip"},{"location":"install/#install-from-source","text":"Clone repository from Github. git clone https://github.com/bionicdl-sustech/DeepClawBenchmark.git cd DeepClawBenchmark Install DeepClaw. python3 setup.py install","title":"Install from source"},{"location":"licence/","text":"Licence \u00b6 MIT License Copyright (c) 2020 BionicDL@SUSTech Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"Licence"},{"location":"licence/#licence","text":"MIT License Copyright (c) 2020 BionicDL@SUSTech Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"Licence"},{"location":"module/","text":"Module Pool \u00b6 DeepClaw maintains a module pool of algorithms and end-to-end robot learning models by integrating some state-of-the-art research results in computer vision and robotics. The codes are placed under deepclaw/modules/. Computation on server \u00b6 As the running environment for each method is different, DeepClaw adopts concepts from cloud robotics. We put the running environments for end-to-end methods which requires heavy computations in a docker container on the server, and deploy the robot control and basic computations on a user computer. Currently we have two servers: Goldenboy and Serbreeze. Currently they are running Ubuntu16.04 and cuda9.0. We plan to upgrade them to Ubuntu18.04 and cuda10 soon. Each user is assigned to have one GPU card by setting environment varible CUDA_VISIBLE_DEVICES. Please don't change it by yourself. If you need more computation resources, please contact us. Goldenby Serbreeze Memory 251.8G 125.8 GiB Processor Intel\u00ae Xeon(R) CPU E5-2698 v4 @ 2.20GHz \u00d7 40 Intel\u00ae Xeon(R) CPU E5-2650 v4 @ 2.20GHz \u00d7 48 GPU Tesla V100 32G x4 GeForce GTX 1080Ti 12G x4 Storage 7.6TB SSD 240G SSD (/home), 960GB SSD+8TB HD (/media/amax/) Users Standard: user-1, user-2, user-3 Standard: student1, student2, student3 IP 10.20.123.35 10.20.73.134 List of modules \u00b6 Segmentation \u00b6 Method Object classes weights Contour detector NA NA Recognition \u00b6 Object Detection \u00b6 Method Object classes weights Efficientdet 204 waste classes link extract code: frra","title":"Module Pool"},{"location":"module/#module-pool","text":"DeepClaw maintains a module pool of algorithms and end-to-end robot learning models by integrating some state-of-the-art research results in computer vision and robotics. The codes are placed under deepclaw/modules/.","title":"Module Pool"},{"location":"module/#computation-on-server","text":"As the running environment for each method is different, DeepClaw adopts concepts from cloud robotics. We put the running environments for end-to-end methods which requires heavy computations in a docker container on the server, and deploy the robot control and basic computations on a user computer. Currently we have two servers: Goldenboy and Serbreeze. Currently they are running Ubuntu16.04 and cuda9.0. We plan to upgrade them to Ubuntu18.04 and cuda10 soon. Each user is assigned to have one GPU card by setting environment varible CUDA_VISIBLE_DEVICES. Please don't change it by yourself. If you need more computation resources, please contact us. Goldenby Serbreeze Memory 251.8G 125.8 GiB Processor Intel\u00ae Xeon(R) CPU E5-2698 v4 @ 2.20GHz \u00d7 40 Intel\u00ae Xeon(R) CPU E5-2650 v4 @ 2.20GHz \u00d7 48 GPU Tesla V100 32G x4 GeForce GTX 1080Ti 12G x4 Storage 7.6TB SSD 240G SSD (/home), 960GB SSD+8TB HD (/media/amax/) Users Standard: user-1, user-2, user-3 Standard: student1, student2, student3 IP 10.20.123.35 10.20.73.134","title":"Computation on server"},{"location":"module/#list-of-modules","text":"","title":"List of modules"},{"location":"module/#segmentation","text":"Method Object classes weights Contour detector NA NA","title":"Segmentation"},{"location":"module/#recognition","text":"","title":"Recognition"},{"location":"module/#object-detection","text":"Method Object classes weights Efficientdet 204 waste classes link extract code: frra","title":"Object Detection"},{"location":"overview/","text":"DeepClaw Overview \u00b6 Design Notes \u00b6 The DeepClaw is a benchmarking module pool that functions as a Reconfigurable Robotic Manipulation System for Robot Learning. Modern systems for robotic manipulation has gradually evolved to a common configuration of a cell station where multiple hardware and software are usually integrated to perform programmable tasks, such as object manipulation, human-robot interaction, and industrial collaboration. The challenge behind most manipulation-based robotic research and application is usually hidden by the growing complexity when integrating related hardware, such as manipulators, grippers, sensors, and environment with objects for manipulation, as well as software, such as algorithms, APIs, drivers, etc., towards a functional robot system. It is widely recognized among industrial robotics that the total cost of integration is usually at the same level as the price tage of the hardware itself, and may become even higher at many scenarios, where the market of robotic integration is actually slightly higher than the selling of robotic hardware itself. For academic research, there are several alternative solutions before DeepClaw. Commercial service providers for robotic integration : This is a common solution that may require the least of effort at the beginning. But in the end, may not be suitable for research purpose at some point where further developement may not be possible or just too expensive. Robot Operating System (ROS) : The development of ROS contributed significantly to the growing adoption of robotic technologies in research and applications, which provides a standardized interface for communication among various robotic devices and software. However, the scope of the ROS is so broad with a relatively steep learning curve for most users, learners, and practioners to get started. Do-it-yourself (DIY) : This is also a common solution which usually starts with the dedicated software provided by the hardware supplier, or if the robotic device is developed by the research team from the ground up. With the growing adoption of collaborative robotic devices, the barrier of usage becomes much lower than before. With a growing interest in robot learning, where learning-based methods are applied to robotic system to solve advanced manipulation tasks towards human-robot interaction. Several questions becomes imminent for researchers and practitioners: Is there a \"common\" setup of robotic manipulation station? By what \"standard\" should I pick the specific hardware to build my robot learning station? Is there a \"pipeline\" of intergration so that I can get the system up-and-running and perform real-world experiment? While there is no \"correct\" answers to questions such as above, these challenges are commonly faced by many researchers already in this field, or have the interest to experiement with robot learning, which give birth to the development of DeepClaw. Based on the existing collection of robotic hardware at the Bionic Design and Learning Lab at the Southern University of Science and Technology , we aim to develop a sharable and reproducible framework of Robot Manipulation System based on reviews of common setups of robot learning stations in recent research, aiming at building up a pool of hardware benchmarking system that facilitates a comparable metric for advanced robot learning algorithms, which is not yet explored in the current research (and also challenging). DeepClaw is hardware-centric module pool towards a common design to benchmark robot stations through a Robotic Manipulation System, which involves a mechanical design of the DeepClaw Station, a growing collection of drivers that supports various robotic hardware, a cloud architecture that supports training and inference of robot learning experiments, and a collection of game objects for task representation. DeepClaw Station Design \u00b6 After reviewing the robot station designs presented in several recent publications on robot manipulation learning, we proposed the following design using a standardized 9090 extrusion from global supplier such as Misumi and a few parts that can be easily machined by local machine shops to build up the system. The design can be accessed interactively through here. Feel free to contact us if you experience trouble sourcing the design, or have any suggestions to improve. AluExtru : 9090 series aluminium extrusion profiles from global supperlier such as Misumi. Misumi Part#: HFS8-9090-630-TPW, HFS8-9090-540-TPW, HFS8-9090-450-TPW. AluPlate : simple design peg-hole style aluminium plate for reliable connection, can be easily machined by you local shop. S2x4, S2x5. FlangeX : simple flange designs that supports heavy load connection, can be easily machined by your local shop. FlangeRobot: Supports Franka, UR, and AUBO in one design, as of now. FlangeFoot: Support the adjustable wheel. FlangeTube: Support RealSense D400 series, as of now. Others : minimum accessories towards flexibility, robustness, and safety. Tube: where cameras are mounted, can be easily modified based on your camera needs. Handle (Part#: GHHD28-A): for ease of handling and safe usage. Adjustable Wheel (GD-80F): for sturdiness and mobility. TableTop (630x540): aluminium with CNC machined, threaded peg-holes, modify as you need. Supported Robotic Hardware \u00b6 A reasonable range of robotic hardware is supported by DeepClaw, with further information explained in the Robot Library section. Pipeline of Integration \u00b6 The overall integration of DeepClaw is generally divided into four stages, with further information explained in the DeepClaw Pipeline section. Server-Client Structure \u00b6 In DeepClaw 2.0, we propose a new structure: the Server-Client Structure. This structure will benefit algorithm modules who require heavy computational resources by enabling GPUs in server. More details can be found here. how to add a new module in server how to create a related module in client Publication \u00b6 arXiv:2005.02588 [cs.RO]","title":"Overview"},{"location":"overview/#deepclaw-overview","text":"","title":"DeepClaw Overview"},{"location":"overview/#design-notes","text":"The DeepClaw is a benchmarking module pool that functions as a Reconfigurable Robotic Manipulation System for Robot Learning. Modern systems for robotic manipulation has gradually evolved to a common configuration of a cell station where multiple hardware and software are usually integrated to perform programmable tasks, such as object manipulation, human-robot interaction, and industrial collaboration. The challenge behind most manipulation-based robotic research and application is usually hidden by the growing complexity when integrating related hardware, such as manipulators, grippers, sensors, and environment with objects for manipulation, as well as software, such as algorithms, APIs, drivers, etc., towards a functional robot system. It is widely recognized among industrial robotics that the total cost of integration is usually at the same level as the price tage of the hardware itself, and may become even higher at many scenarios, where the market of robotic integration is actually slightly higher than the selling of robotic hardware itself. For academic research, there are several alternative solutions before DeepClaw. Commercial service providers for robotic integration : This is a common solution that may require the least of effort at the beginning. But in the end, may not be suitable for research purpose at some point where further developement may not be possible or just too expensive. Robot Operating System (ROS) : The development of ROS contributed significantly to the growing adoption of robotic technologies in research and applications, which provides a standardized interface for communication among various robotic devices and software. However, the scope of the ROS is so broad with a relatively steep learning curve for most users, learners, and practioners to get started. Do-it-yourself (DIY) : This is also a common solution which usually starts with the dedicated software provided by the hardware supplier, or if the robotic device is developed by the research team from the ground up. With the growing adoption of collaborative robotic devices, the barrier of usage becomes much lower than before. With a growing interest in robot learning, where learning-based methods are applied to robotic system to solve advanced manipulation tasks towards human-robot interaction. Several questions becomes imminent for researchers and practitioners: Is there a \"common\" setup of robotic manipulation station? By what \"standard\" should I pick the specific hardware to build my robot learning station? Is there a \"pipeline\" of intergration so that I can get the system up-and-running and perform real-world experiment? While there is no \"correct\" answers to questions such as above, these challenges are commonly faced by many researchers already in this field, or have the interest to experiement with robot learning, which give birth to the development of DeepClaw. Based on the existing collection of robotic hardware at the Bionic Design and Learning Lab at the Southern University of Science and Technology , we aim to develop a sharable and reproducible framework of Robot Manipulation System based on reviews of common setups of robot learning stations in recent research, aiming at building up a pool of hardware benchmarking system that facilitates a comparable metric for advanced robot learning algorithms, which is not yet explored in the current research (and also challenging). DeepClaw is hardware-centric module pool towards a common design to benchmark robot stations through a Robotic Manipulation System, which involves a mechanical design of the DeepClaw Station, a growing collection of drivers that supports various robotic hardware, a cloud architecture that supports training and inference of robot learning experiments, and a collection of game objects for task representation.","title":"Design Notes"},{"location":"overview/#deepclaw-station-design","text":"After reviewing the robot station designs presented in several recent publications on robot manipulation learning, we proposed the following design using a standardized 9090 extrusion from global supplier such as Misumi and a few parts that can be easily machined by local machine shops to build up the system. The design can be accessed interactively through here. Feel free to contact us if you experience trouble sourcing the design, or have any suggestions to improve. AluExtru : 9090 series aluminium extrusion profiles from global supperlier such as Misumi. Misumi Part#: HFS8-9090-630-TPW, HFS8-9090-540-TPW, HFS8-9090-450-TPW. AluPlate : simple design peg-hole style aluminium plate for reliable connection, can be easily machined by you local shop. S2x4, S2x5. FlangeX : simple flange designs that supports heavy load connection, can be easily machined by your local shop. FlangeRobot: Supports Franka, UR, and AUBO in one design, as of now. FlangeFoot: Support the adjustable wheel. FlangeTube: Support RealSense D400 series, as of now. Others : minimum accessories towards flexibility, robustness, and safety. Tube: where cameras are mounted, can be easily modified based on your camera needs. Handle (Part#: GHHD28-A): for ease of handling and safe usage. Adjustable Wheel (GD-80F): for sturdiness and mobility. TableTop (630x540): aluminium with CNC machined, threaded peg-holes, modify as you need.","title":"DeepClaw Station Design"},{"location":"overview/#supported-robotic-hardware","text":"A reasonable range of robotic hardware is supported by DeepClaw, with further information explained in the Robot Library section.","title":"Supported Robotic Hardware"},{"location":"overview/#pipeline-of-integration","text":"The overall integration of DeepClaw is generally divided into four stages, with further information explained in the DeepClaw Pipeline section.","title":"Pipeline of Integration"},{"location":"overview/#server-client-structure","text":"In DeepClaw 2.0, we propose a new structure: the Server-Client Structure. This structure will benefit algorithm modules who require heavy computational resources by enabling GPUs in server. More details can be found here. how to add a new module in server how to create a related module in client","title":"Server-Client Structure"},{"location":"overview/#publication","text":"arXiv:2005.02588 [cs.RO]","title":"Publication"},{"location":"pipeline/","text":"Standardized Process in DeepClaw \u00b6 In DeepClaw, a sub-task is defined by a pipeline of modules, including segmentation, recognition, grasp planning, and motion planning, as shown in above figure. The pipeline takes color/depth images, force feedback, hardware limitation, and environment information as input and gives actions to the manipulation system and pushes data and results to data monitor. Segmentation \u00b6 Segmentation and recognition involve analyzing information gained from the perception system. Segmentation is the process that robot cell collecting environment information and representing spatial information of the target objects by using perception algorithms. The output of the segmentation module can be pixel-wise masks or bounding boxes. DeepClaw includes basic segmentation based on contour and edge detection in OpenCV. Code | Segmentation Module Pool Recognition \u00b6 Recognition is the process of extracting features of the target object beyond location information. In this step, the robot cell infers the category of the target object by applying specific methods, such as support vector machine(SVM) and convolutional neural network. Some of the end-to-end neural networks infer the location and category of the target object at the same time. Code | Recognition Module Pool Grasping Planning \u00b6 Grasping planning aims to find the optimal pose for the robot arm and end-effect to approach the target objects, which is highly dependent on both the end-effector and the objects. Recent years, research interests have shifted from analytic methods to data-driven methods. DeepClaw has implemented an end-to-end grasp planning model based on fully convolutional AlexNet, which was trained on 5000 random grasps with labels. Code | Grasping Planning Module Pool Motion Planning \u00b6 Motion planning utilizes information above, such as grasping pose, force sensor data, constrain of the robot system, and limitation of working space, to obtain collision-free trajectories. Currently, waypoint-based motion planning is used through our tasks. For UR5 and UR10e, we utilize the movej command implemented in UR\u2019s controller to plan and execute a path between waypoints. For Franka, we utilize a fourthorder motion generator in the joint space provided by the libfranka software. Code | Motion Planning Module Pool End-to-End \u00b6 Actually, many learning algorithms may implement multiple functions, like SSD for both localization and recognition. So, the sub-task process may transforms to three or two, even only one stage process. Code | End-to-End","title":"DeepClaw Pipeline"},{"location":"pipeline/#standardized-process-in-deepclaw","text":"In DeepClaw, a sub-task is defined by a pipeline of modules, including segmentation, recognition, grasp planning, and motion planning, as shown in above figure. The pipeline takes color/depth images, force feedback, hardware limitation, and environment information as input and gives actions to the manipulation system and pushes data and results to data monitor.","title":"Standardized Process in DeepClaw"},{"location":"pipeline/#segmentation","text":"Segmentation and recognition involve analyzing information gained from the perception system. Segmentation is the process that robot cell collecting environment information and representing spatial information of the target objects by using perception algorithms. The output of the segmentation module can be pixel-wise masks or bounding boxes. DeepClaw includes basic segmentation based on contour and edge detection in OpenCV. Code | Segmentation Module Pool","title":"Segmentation"},{"location":"pipeline/#recognition","text":"Recognition is the process of extracting features of the target object beyond location information. In this step, the robot cell infers the category of the target object by applying specific methods, such as support vector machine(SVM) and convolutional neural network. Some of the end-to-end neural networks infer the location and category of the target object at the same time. Code | Recognition Module Pool","title":"Recognition"},{"location":"pipeline/#grasping-planning","text":"Grasping planning aims to find the optimal pose for the robot arm and end-effect to approach the target objects, which is highly dependent on both the end-effector and the objects. Recent years, research interests have shifted from analytic methods to data-driven methods. DeepClaw has implemented an end-to-end grasp planning model based on fully convolutional AlexNet, which was trained on 5000 random grasps with labels. Code | Grasping Planning Module Pool","title":"Grasping Planning"},{"location":"pipeline/#motion-planning","text":"Motion planning utilizes information above, such as grasping pose, force sensor data, constrain of the robot system, and limitation of working space, to obtain collision-free trajectories. Currently, waypoint-based motion planning is used through our tasks. For UR5 and UR10e, we utilize the movej command implemented in UR\u2019s controller to plan and execute a path between waypoints. For Franka, we utilize a fourthorder motion generator in the joint space provided by the libfranka software. Code | Motion Planning Module Pool","title":"Motion Planning"},{"location":"pipeline/#end-to-end","text":"Actually, many learning algorithms may implement multiple functions, like SSD for both localization and recognition. So, the sub-task process may transforms to three or two, even only one stage process. Code | End-to-End","title":"End-to-End"},{"location":"release/","text":"Release Note \u00b6 v2.0 \u00b6 DeepClaw with server-client support. - move to Python 3.7 - update with server-client support - add trash sorting project - change the way loading configuration - fixed some bugs of drivers v1.5 \u00b6 DeepClaw released version. - users can install DeepClaw by using pip v1.0 \u00b6 The first version of DeepClaw - Based on Python 2.7 - UR10e, UR5, Realsense D435, and Realsense D435i support","title":"Release Note"},{"location":"release/#release-note","text":"","title":"Release Note"},{"location":"release/#v20","text":"DeepClaw with server-client support. - move to Python 3.7 - update with server-client support - add trash sorting project - change the way loading configuration - fixed some bugs of drivers","title":"v2.0"},{"location":"release/#v15","text":"DeepClaw released version. - users can install DeepClaw by using pip","title":"v1.5"},{"location":"release/#v10","text":"The first version of DeepClaw - Based on Python 2.7 - UR10e, UR5, Realsense D435, and Realsense D435i support","title":"v1.0"},{"location":"robot/","text":"Robot Library \u00b6 The DeepClaw cell, shown on the bottom of the following figure, is a self-contained robot cell for manipulation tasks, including an assembled station, a robot arm, a robot end-effector, a visual sensor. Thanks to the standardized design, we are able to buy the components and assemble the robot station quickly. The hardware setup is defined in the software part of DeepClaw through configuration files. The same task should be easily reproduced on different hardware setups if the configuration file is adequately defined. Arm (Manipulator) \u00b6 Universal Robots: UR10e \u00b6 Universal Robots: UR5 (TODO) \u00b6 Franka Emika: Panda (TODO) \u00b6 AUBO: i5 (TODO) \u00b6 Denso: Cobotta (TODO) \u00b6 Yaskawa: MotoMini (TODO) \u00b6 Hand (Gripper) \u00b6 Robotiq: HandE \u00b6 OnRobot: RG6 (TODO) \u00b6 Customized: Suction (TODO) \u00b6 Eye (Camera) \u00b6 Intel: Realsense D435 \u00b6 Intel: Realsense D435i (TODO) \u00b6 Microsoft: Kinect Azure (TODO) \u00b6 Microsoft: Kinect V2 (TODO) \u00b6 Photoneo: Model M (TODO) \u00b6 Accessory (Sensor) \u00b6 OnRobot: Force-Torque Sensors (TODO) \u00b6 ATI: nano17 (TODO) \u00b6 DeepClaw Stations with UR10e (TODO) \u00b6 UR10e+HandE+D435 (TODO) \u00b6 UR10e+HandE+Azure (TODO) \u00b6 UR10e+HandE+PhotoM (TODO) \u00b6 DeepClaw Stations with UR5 (TODO) \u00b6 UR5+RG6+D435 (TODO) \u00b6 DeepClaw Stations with Franka (TODO) \u00b6 Franka+2Finger+D435 (TODO) \u00b6 DeepClaw Stations with MotoMini (TODO) \u00b6 MotoMini+Suction+D435 (TODO) \u00b6 DeepClaw Stations with Cobotta (TODO) \u00b6 Cobotta+2Finger+D435 (TODO) \u00b6 DeepClaw Stations with AUBOi5 (TODO) \u00b6 AUBOi5+Suction+D435 (TODO) \u00b6","title":"Robot Library"},{"location":"robot/#robot-library","text":"The DeepClaw cell, shown on the bottom of the following figure, is a self-contained robot cell for manipulation tasks, including an assembled station, a robot arm, a robot end-effector, a visual sensor. Thanks to the standardized design, we are able to buy the components and assemble the robot station quickly. The hardware setup is defined in the software part of DeepClaw through configuration files. The same task should be easily reproduced on different hardware setups if the configuration file is adequately defined.","title":"Robot Library"},{"location":"robot/#arm-manipulator","text":"","title":"Arm (Manipulator)"},{"location":"robot/#universal-robots-ur10e","text":"","title":"Universal Robots: UR10e"},{"location":"robot/#universal-robots-ur5-todo","text":"","title":"Universal Robots: UR5 (TODO)"},{"location":"robot/#franka-emika-panda-todo","text":"","title":"Franka Emika: Panda (TODO)"},{"location":"robot/#aubo-i5-todo","text":"","title":"AUBO: i5 (TODO)"},{"location":"robot/#denso-cobotta-todo","text":"","title":"Denso: Cobotta (TODO)"},{"location":"robot/#yaskawa-motomini-todo","text":"","title":"Yaskawa: MotoMini (TODO)"},{"location":"robot/#hand-gripper","text":"","title":"Hand (Gripper)"},{"location":"robot/#robotiq-hande","text":"","title":"Robotiq: HandE"},{"location":"robot/#onrobot-rg6-todo","text":"","title":"OnRobot: RG6 (TODO)"},{"location":"robot/#customized-suction-todo","text":"","title":"Customized: Suction (TODO)"},{"location":"robot/#eye-camera","text":"","title":"Eye (Camera)"},{"location":"robot/#intel-realsense-d435","text":"","title":"Intel: Realsense D435"},{"location":"robot/#intel-realsense-d435i-todo","text":"","title":"Intel: Realsense D435i (TODO)"},{"location":"robot/#microsoft-kinect-azure-todo","text":"","title":"Microsoft: Kinect Azure (TODO)"},{"location":"robot/#microsoft-kinect-v2-todo","text":"","title":"Microsoft: Kinect V2 (TODO)"},{"location":"robot/#photoneo-model-m-todo","text":"","title":"Photoneo: Model M (TODO)"},{"location":"robot/#accessory-sensor","text":"","title":"Accessory (Sensor)"},{"location":"robot/#onrobot-force-torque-sensors-todo","text":"","title":"OnRobot: Force-Torque Sensors (TODO)"},{"location":"robot/#ati-nano17-todo","text":"","title":"ATI: nano17 (TODO)"},{"location":"robot/#deepclaw-stations-with-ur10e-todo","text":"","title":"DeepClaw Stations with UR10e (TODO)"},{"location":"robot/#ur10ehanded435-todo","text":"","title":"UR10e+HandE+D435 (TODO)"},{"location":"robot/#ur10ehandeazure-todo","text":"","title":"UR10e+HandE+Azure (TODO)"},{"location":"robot/#ur10ehandephotom-todo","text":"","title":"UR10e+HandE+PhotoM (TODO)"},{"location":"robot/#deepclaw-stations-with-ur5-todo","text":"","title":"DeepClaw Stations with UR5 (TODO)"},{"location":"robot/#ur5rg6d435-todo","text":"","title":"UR5+RG6+D435 (TODO)"},{"location":"robot/#deepclaw-stations-with-franka-todo","text":"","title":"DeepClaw Stations with Franka (TODO)"},{"location":"robot/#franka2fingerd435-todo","text":"","title":"Franka+2Finger+D435 (TODO)"},{"location":"robot/#deepclaw-stations-with-motomini-todo","text":"","title":"DeepClaw Stations with MotoMini (TODO)"},{"location":"robot/#motominisuctiond435-todo","text":"","title":"MotoMini+Suction+D435 (TODO)"},{"location":"robot/#deepclaw-stations-with-cobotta-todo","text":"","title":"DeepClaw Stations with Cobotta (TODO)"},{"location":"robot/#cobotta2fingerd435-todo","text":"","title":"Cobotta+2Finger+D435 (TODO)"},{"location":"robot/#deepclaw-stations-with-auboi5-todo","text":"","title":"DeepClaw Stations with AUBOi5 (TODO)"},{"location":"robot/#auboi5suctiond435-todo","text":"","title":"AUBOi5+Suction+D435 (TODO)"},{"location":"sim2real/","text":"Sim-2-Real \u00b6 Introduction to CoppeliaSim (V-REP) and PyRep \u00b6 We use CoppeliaSim and PyRep for the simulation in deepclaw. The robot simulator CoppeliaSim, with integrated development environment, is based on a distributed control architecture: each object/model can be individually controlled via an embedded script, a plugin, a ROS or BlueZero node, a remote API client, or a custom solution. This makes CoppeliaSim very versatile and ideal for multi-robot applications. Controllers can be written in C/C++, Python, Java, Lua, Matlab or Octave. PyRep is a toolkit for robot learning research, built on top of CoppeliaSim (previously called V-REP). Please refer to the deepclaw/sim2real/simulation folder for installation instructions and usage examples. Project 1: Kinematic Picking in PyRep \u00b6 The Kinematic picking project aims to build a simple pick and place scene in CoppeliaSim and complete the pick and place task without using any vision input. The project also demonstrates how to generate a predefined path and let the Franka arm go through the waypoints. Please refer to the deepclaw/sim2real/simulation/tasks folder for instructions. Project 2: Simulated Robot Player \u00b6 The project aims to build a robot player of Tic-Tac-Toe in simulation. The V-rep Scene file is provided and for making robot move and play with human, we have to implement 3 basic elements: Computer Vision Try to recognize the object on the desk, classify the category of object and calculate the real world position of object via depth image and the previous result you got. It is essential information you should know if you want your virtual robot put the chess on the correct position. The decision maker(Minmax or reinforcement learning method) The robot need to know where to put the chess that can lead it go to win. The classical method MINMAX is recommended and the reinforcement learning method that based on MDP is happily welcomed. Robot control Control robot to run on the trajectory that lead robot gripper grasp chess and release chess stable and robust. Project 3: Claw Machine \u00b6 The project aims to program a franka robot to claw the toys in front of the robot and put them to a box next to robot. Please use the graspNet model in DeepClaw, which is an end2end model that take an image as input and output the best position and pose to pick. Please read our paper for a detailed explanation of the grasping neural network.","title":"Sim-2-Real"},{"location":"sim2real/#sim-2-real","text":"","title":"Sim-2-Real"},{"location":"sim2real/#introduction-to-coppeliasim-v-rep-and-pyrep","text":"We use CoppeliaSim and PyRep for the simulation in deepclaw. The robot simulator CoppeliaSim, with integrated development environment, is based on a distributed control architecture: each object/model can be individually controlled via an embedded script, a plugin, a ROS or BlueZero node, a remote API client, or a custom solution. This makes CoppeliaSim very versatile and ideal for multi-robot applications. Controllers can be written in C/C++, Python, Java, Lua, Matlab or Octave. PyRep is a toolkit for robot learning research, built on top of CoppeliaSim (previously called V-REP). Please refer to the deepclaw/sim2real/simulation folder for installation instructions and usage examples.","title":"Introduction to CoppeliaSim (V-REP) and PyRep"},{"location":"sim2real/#project-1-kinematic-picking-in-pyrep","text":"The Kinematic picking project aims to build a simple pick and place scene in CoppeliaSim and complete the pick and place task without using any vision input. The project also demonstrates how to generate a predefined path and let the Franka arm go through the waypoints. Please refer to the deepclaw/sim2real/simulation/tasks folder for instructions.","title":"Project 1: Kinematic Picking in PyRep"},{"location":"sim2real/#project-2-simulated-robot-player","text":"The project aims to build a robot player of Tic-Tac-Toe in simulation. The V-rep Scene file is provided and for making robot move and play with human, we have to implement 3 basic elements: Computer Vision Try to recognize the object on the desk, classify the category of object and calculate the real world position of object via depth image and the previous result you got. It is essential information you should know if you want your virtual robot put the chess on the correct position. The decision maker(Minmax or reinforcement learning method) The robot need to know where to put the chess that can lead it go to win. The classical method MINMAX is recommended and the reinforcement learning method that based on MDP is happily welcomed. Robot control Control robot to run on the trajectory that lead robot gripper grasp chess and release chess stable and robust.","title":"Project 2: Simulated Robot Player"},{"location":"sim2real/#project-3-claw-machine","text":"The project aims to program a franka robot to claw the toys in front of the robot and put them to a box next to robot. Please use the graspNet model in DeepClaw, which is an end2end model that take an image as input and output the best position and pose to pick. Please read our paper for a detailed explanation of the grasping neural network.","title":"Project 3: Claw Machine"},{"location":"task/","text":"Task Family \u00b6 In DeepClaw, a manipulation task involves three hierarchical concepts: task, sub-task, functionality module. A task protocol should clearly define the main purpose of the task, the target objects, the robot, and hardware setup, procedures to fulfill the task and execution constraints [19]. Each task may consist of several repetitive sub-tasks. A pipeline of functional modules can accomplish each sub-task. The most similarity between game and dexterous manipulations enable reproducible experiments in various environment. All of the game manipulation tasks can be classified from two different perspectives: spatial reasoning and temporal reasoning. Compared with human daily dexterous manipulations, game manipulations have a noticeable distinction in spatial and temporal dimensions. \"Jigsaw Puzzle,\" for example, requires a meaningful pattern at finally by placing certain pieces in settled spatial position and orientation using robot cell. We can summarize that \"Jigsaw Puzzle\" focuses on spatial reasoning rather than temporal reasoning sine chronological operations to finish the puzzle are needless during the whole placing process. \"Tic-tac-toe Game\" is the contrary that emphasizes moving chess chronologically rather than its spatial position and orientation (distinguish the type of pieces rather than each piece individual). Claw machine is another popular game that involves picking and placing to clear the toy tray. We hypothesize that both robot cells and intelligent algorithms lead to performance differences when executing game manipulation tasks. Waste Sorting: Detector of Daily Life Waste \u00b6 With the development of the economy, we are producing more daily life waste than ever in history. Waste sorting can help improve the recycling of renewable resources. The waste sorting task aims to train a waste detector and build a automatic waste sorting station using robot arms. The training dataset is from 2020 Haihua AI Challange - Garbage classification. A description can be found under DeepClawBenchmark/data/Haihua-Waste-Sorting. In the demo, we use a UR10e, a Realsense D435 camera, a HandE with self-designed soft fingers to build a automatic waste sorting station. Please refer to the project page (DeepClawBenchmark/projects/proj_WasteSorting) for more details. Tic-Tac-Toe: Board Games as Adversarial Interaction \u00b6 Tic-Tac-Toe game is a temporal reasoning related task, which required two players moving pieces alternately. To simplify this game as a baseline, the two players use the same placing strategy, namely the Minimax algorithm with depth 3, and are both executed by the robot arm. We use green and blue cubes from Yale-CMU-Berkeley objects set [19] representing two types of pieces. At the start of the game, 3\u00d73 checkerboards printed on an A4 paper is placed in front of the robot base, and the two types of pieces are lined on the left and right side of the chessboard as shown in Fig 4(a). The task is to pick a type of piece and place it on one of nine boxes on the checkerboard in turns until one player wins or ends with a tie. Claw Machine: End-to-End Manipulation Benchmarking \u00b6 This benchmark measures the performance of a learned policy for predicting robust grasps over different robot cells. At the start of the task, a 60cm\u00d770cm white bin stuffed by eight soft toys and an empty 30cm\u00d740cm blue bin are placed side by side on the table top as shown in the following figure. The task is to transport the toys to the blue bin one by one until clearing the white bin. We restrict the gripper to grasp vertically, allowing only rotations along the z-axis of the robot base. Jigsaw Puzzle: Tiling Game for Modular Benchmarking \u00b6 A jigsaw puzzle is a tiling game that requires the assembly of often oddly shaped interlocking and tessellating pieces. The jigsaw set used in this paper contains four thin wooden pieces with an image printed on one side and can form a 10.2cm\u00d710.2cm picture when they are correctly assembled. We use a suction cup to complete the task on all three robot cells as the jigsaw piece is only 5 mm thick and is too challenging for grippers. At the start of the task, the four pieces are randomly placed on the table top, as shown in Fig. 6(a). The task is to detect and pick one jigsaw piece at a time and place it at the required location according to its shape and texture information, and finally assemble all the four pieces into one whole piece. We restrict the gripper to pick vertically, allowing only rotations along the z-axis of the robot base.","title":"Task Family"},{"location":"task/#task-family","text":"In DeepClaw, a manipulation task involves three hierarchical concepts: task, sub-task, functionality module. A task protocol should clearly define the main purpose of the task, the target objects, the robot, and hardware setup, procedures to fulfill the task and execution constraints [19]. Each task may consist of several repetitive sub-tasks. A pipeline of functional modules can accomplish each sub-task. The most similarity between game and dexterous manipulations enable reproducible experiments in various environment. All of the game manipulation tasks can be classified from two different perspectives: spatial reasoning and temporal reasoning. Compared with human daily dexterous manipulations, game manipulations have a noticeable distinction in spatial and temporal dimensions. \"Jigsaw Puzzle,\" for example, requires a meaningful pattern at finally by placing certain pieces in settled spatial position and orientation using robot cell. We can summarize that \"Jigsaw Puzzle\" focuses on spatial reasoning rather than temporal reasoning sine chronological operations to finish the puzzle are needless during the whole placing process. \"Tic-tac-toe Game\" is the contrary that emphasizes moving chess chronologically rather than its spatial position and orientation (distinguish the type of pieces rather than each piece individual). Claw machine is another popular game that involves picking and placing to clear the toy tray. We hypothesize that both robot cells and intelligent algorithms lead to performance differences when executing game manipulation tasks.","title":"Task Family"},{"location":"task/#waste-sorting-detector-of-daily-life-waste","text":"With the development of the economy, we are producing more daily life waste than ever in history. Waste sorting can help improve the recycling of renewable resources. The waste sorting task aims to train a waste detector and build a automatic waste sorting station using robot arms. The training dataset is from 2020 Haihua AI Challange - Garbage classification. A description can be found under DeepClawBenchmark/data/Haihua-Waste-Sorting. In the demo, we use a UR10e, a Realsense D435 camera, a HandE with self-designed soft fingers to build a automatic waste sorting station. Please refer to the project page (DeepClawBenchmark/projects/proj_WasteSorting) for more details.","title":"Waste Sorting: Detector of Daily Life Waste"},{"location":"task/#tic-tac-toe-board-games-as-adversarial-interaction","text":"Tic-Tac-Toe game is a temporal reasoning related task, which required two players moving pieces alternately. To simplify this game as a baseline, the two players use the same placing strategy, namely the Minimax algorithm with depth 3, and are both executed by the robot arm. We use green and blue cubes from Yale-CMU-Berkeley objects set [19] representing two types of pieces. At the start of the game, 3\u00d73 checkerboards printed on an A4 paper is placed in front of the robot base, and the two types of pieces are lined on the left and right side of the chessboard as shown in Fig 4(a). The task is to pick a type of piece and place it on one of nine boxes on the checkerboard in turns until one player wins or ends with a tie.","title":"Tic-Tac-Toe: Board Games as Adversarial Interaction"},{"location":"task/#claw-machine-end-to-end-manipulation-benchmarking","text":"This benchmark measures the performance of a learned policy for predicting robust grasps over different robot cells. At the start of the task, a 60cm\u00d770cm white bin stuffed by eight soft toys and an empty 30cm\u00d740cm blue bin are placed side by side on the table top as shown in the following figure. The task is to transport the toys to the blue bin one by one until clearing the white bin. We restrict the gripper to grasp vertically, allowing only rotations along the z-axis of the robot base.","title":"Claw Machine: End-to-End Manipulation Benchmarking"},{"location":"task/#jigsaw-puzzle-tiling-game-for-modular-benchmarking","text":"A jigsaw puzzle is a tiling game that requires the assembly of often oddly shaped interlocking and tessellating pieces. The jigsaw set used in this paper contains four thin wooden pieces with an image printed on one side and can form a 10.2cm\u00d710.2cm picture when they are correctly assembled. We use a suction cup to complete the task on all three robot cells as the jigsaw piece is only 5 mm thick and is too challenging for grippers. At the start of the task, the four pieces are randomly placed on the table top, as shown in Fig. 6(a). The task is to detect and pick one jigsaw piece at a time and place it at the required location according to its shape and texture information, and finally assemble all the four pieces into one whole piece. We restrict the gripper to pick vertically, allowing only rotations along the z-axis of the robot base.","title":"Jigsaw Puzzle: Tiling Game for Modular Benchmarking"}]}